{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Techniques\n",
        "To learn more about using Ensemble Methods, we are going to use the 'ML Marathon' dataset to test the results of XGBoost, Random Forest, and AdaBoost. All of which are different ensemble packages that will classify our data and return an accuracy score depending on the test data partition. \n",
        "\n",
        "Before we start implementing those ensemble methods, we first have to do some exploratory data analysis and examine how can we clean our data to better fit the methods we are about to use. Here's a first look at our dataset."
      ],
      "metadata": {
        "id": "-hDGXwCejLg3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDbZXb8iLuoO",
        "outputId": "6a5c06f6-9f9c-4fa6-b11f-39eda5e2dc72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age          job  marital  education default  balance housing loan  \\\n",
            "0   38   technician  married   tertiary      no      127     yes   no   \n",
            "1   41    housemaid  married    primary      no      365      no   no   \n",
            "2   39   management   single   tertiary      no     2454     yes   no   \n",
            "3   49  blue-collar  married    primary      no     6215     yes   no   \n",
            "4   37     services  married  secondary      no     1694     yes  yes   \n",
            "\n",
            "    contact  day month  duration  campaign  pdays  previous poutcome deposit  \n",
            "0  cellular   14   oct       113         1     50         2  success      no  \n",
            "1  cellular    8   aug       203         5     -1         0  unknown      no  \n",
            "2  cellular    4   may       716         3    263         2  failure     yes  \n",
            "3  cellular   11   may       549         1     -1         0  unknown      no  \n",
            "4  cellular   29   jan       404         2    251         6  failure      no  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8371 entries, 0 to 8370\n",
            "Data columns (total 17 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   age        8371 non-null   int64 \n",
            " 1   job        8371 non-null   object\n",
            " 2   marital    8371 non-null   object\n",
            " 3   education  8371 non-null   object\n",
            " 4   default    8371 non-null   object\n",
            " 5   balance    8371 non-null   int64 \n",
            " 6   housing    8371 non-null   object\n",
            " 7   loan       8371 non-null   object\n",
            " 8   contact    8371 non-null   object\n",
            " 9   day        8371 non-null   int64 \n",
            " 10  month      8371 non-null   object\n",
            " 11  duration   8371 non-null   int64 \n",
            " 12  campaign   8371 non-null   int64 \n",
            " 13  pdays      8371 non-null   int64 \n",
            " 14  previous   8371 non-null   int64 \n",
            " 15  poutcome   8371 non-null   object\n",
            " 16  deposit    8371 non-null   object\n",
            "dtypes: int64(7), object(10)\n",
            "memory usage: 1.1+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('data.csv')\n",
        "print(df.head())\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis\n",
        "The first part of our EDA is to see if there are any missing values and drop columns if necessary to maintain the validity of our results. In this case, there are none so nothing needs to be done."
      ],
      "metadata": {
        "id": "5G_uVCsKQGZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "na_values = df.isna().mean(axis=0)\n",
        "print(na_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2PdZy5HPtpt",
        "outputId": "1cba67bc-5875-4531-c4d1-b7b315651061"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age          0.0\n",
            "job          0.0\n",
            "marital      0.0\n",
            "education    0.0\n",
            "default      0.0\n",
            "balance      0.0\n",
            "housing      0.0\n",
            "loan         0.0\n",
            "contact      0.0\n",
            "day          0.0\n",
            "month        0.0\n",
            "duration     0.0\n",
            "campaign     0.0\n",
            "pdays        0.0\n",
            "previous     0.0\n",
            "poutcome     0.0\n",
            "deposit      0.0\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we designate a feature to be the target to which the classification will be executed on, store it, and drop it from the actual data. "
      ],
      "metadata": {
        "id": "8RoNi5jtQW2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropped = df.drop(\"deposit\", axis=1)\n",
        "target = df.deposit"
      ],
      "metadata": {
        "id": "-O0joRBiQXD1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For better utilization of ensemble algorithms, we have to scale all of our numerical data and factor categorical data. We will do this by creating a pipeline that will factor all the categorical data and another pipeline that will scale all the numerical data to have a mean of 0 and a variance of 1. "
      ],
      "metadata": {
        "id": "Ft1GjaM_Q5t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "catpipe = Pipeline(\n",
        "    steps=[\n",
        "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"oh-encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "numpipe = Pipeline(\n",
        "    steps=[(\"impute\", SimpleImputer(strategy=\"mean\")), \n",
        "           (\"scale\", StandardScaler())]\n",
        ")"
      ],
      "metadata": {
        "id": "6srJvwYeQ5Gg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's combine both pipelines and apply it to all of the data. "
      ],
      "metadata": {
        "id": "UrpDOIbAREUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "categorical_data = dropped.select_dtypes(exclude=\"number\").columns\n",
        "numerical_data = dropped.select_dtypes(include=\"number\").columns\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "full_processor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"numeric\", numpipe, numerical_data),\n",
        "        (\"categorical\", catpipe, categorical_data),\n",
        "    ]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "5rCm6d0jREjA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBoost\n",
        "Now that we have prepared our data we are going to import XGBoost, process the previously analyzed data and split it into train and test, then examine the XGBoost algorithm's accuracy using default hyper-parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "wheGJcEHLyCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "xgboost_class = xgb.XGBClassifier()\n",
        "\n",
        "dep_proc = full_processor.fit_transform(dropped)\n",
        "indep_proc = SimpleImputer(strategy=\"most_frequent\").fit_transform(\n",
        "    target.values.reshape(-1, 1)\n",
        ")\n",
        "\n",
        "# split the data into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "dep_train, dep_test, indep_train, indep_test = train_test_split(\n",
        "    dep_proc, indep_proc, stratify=indep_proc, random_state=1121218\n",
        ")\n",
        "\n",
        "# make predictions based on model, find accuracy score and print it\n",
        "# measure accuracy and time to make the predictions\n",
        "from sklearn.metrics import accuracy_score\n",
        "xgboost_class.fit(dep_train, indep_train)\n",
        "import time\n",
        "start_time = time.time()\n",
        "preds = xgboost_class.predict(dep_test)\n",
        "print(\"--- %.6s seconds ---\" % (time.time() - start_time))\n",
        "print('Accuracy: ', accuracy_score(indep_test, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRZN-06DM2Us",
        "outputId": "dc9bd65d-2cb8-41a5-90af-4a93714321a9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 0.0076 seconds ---\n",
            "Accuracy:  0.8413760152890588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest\n",
        "The next ensemble algorithm we are going to use is Random Forest. Lucky for us, this algorithm is very easy to implement since it's found in the widely-used sklearn package. We've already done the hard work of cleaning our data and splitting it into train and test so all we have to do is use the sklearn package to make predictions and find the accuracy score for random forest. "
      ],
      "metadata": {
        "id": "yJE0FEIrUrlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_class = RandomForestClassifier(n_estimators = 100) \n",
        "\n",
        "rf_class.fit(dep_train, indep_train)\n",
        " \n",
        "# make predictions based on model, find accuracy score and print it\n",
        "# measure accuracy and time to make the predictions\n",
        "import time\n",
        "start_time = time.time()\n",
        "rf_pred = rf_class.predict(dep_test)\n",
        "print(\"--- %.6s seconds ---\" % (time.time() - start_time))\n",
        "from sklearn.metrics import accuracy_score\n",
        "print('Accuracy: ', accuracy_score(indep_test, rf_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2AKi1d0mDNf",
        "outputId": "7243e3c6-b436-4d0a-f152-455fccefe8a3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 0.0595 seconds ---\n",
            "Accuracy:  0.8394648829431438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaBoost\n",
        "\n",
        "The last algorithm we are going to use is AdaBoost. This algorithm is similar to Random Forest in a lot of ways except that it's decision trees only have a dpeth of 1. To analyze the accuracy of this algorithm we are going to do the same thing we did with Random Forest but using the AdaBoost libraries instead. "
      ],
      "metadata": {
        "id": "dprQDMwvnfcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "adb_class = AdaBoostClassifier()\n",
        "adb_class.fit(dep_train, indep_train)\n",
        "\n",
        "# measure accuracy and time to make the predictions\n",
        "import time\n",
        "start_time = time.time()\n",
        "adb_pred = adb_class.predict(dep_test)\n",
        "print(\"--- %.6s seconds ---\" % (time.time() - start_time))\n",
        "from sklearn.metrics import accuracy_score\n",
        "print('Accuracy: ', accuracy_score(indep_test, adb_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VryBh_Ionxj",
        "outputId": "01161224-19bd-4128-85ea-bddc8d9cdf17"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 0.0350 seconds ---\n",
            "Accuracy:  0.831820353559484\n"
          ]
        }
      ]
    }
  ]
}